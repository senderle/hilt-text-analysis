---
layout: post
title:  "HILT Text Analysis 2016: Syllabus"
date:   2016-07-08 12:38:37 -0400
categories: syllabus
---

## **Day 1:**
* **Session 1: Introductions**
    1. Introductions
    2. Course plan
    3. What computers see
* **Session 2: Intersections**
    1. Building a bridge between humanistic and computational assumptions about language
    2. Humanities research questions and computational methods <!-- [[ppt](https://github.com/senderle/hilt-text-analysis/blob/master/Humanities%20research%20questions%20HILT%202016.pptx?raw=true)] -->
* **Session 3: Designing and Building a Corpus**
    1. Curating data sets
    2. Exercise: Designing a corpus, an intellectual pursuit [[pdf](https://github.com/senderle/hilt-text-analysis/blob/master/Making%20a%20Data%20Set%20-%20HILT%202016.pdf)]
    2. Exercise: Building a corpus, a techno-social pursuit [[pdf](https://github.com/senderle/hilt-text-analysis/blob/master/Making%20a%20Data%20Set%20-%20HILT%202016.pdf)]

## **Day 2:**
* **Session 1: Collecting Data**
    1. Using an API: Chronicling America [[jupyter-notebook](https://github.com/senderle/hilt-text-analysis/blob/master/Chronicling%20America%20API.ipynb)]
    2. Scraping: Wikipedia + Beautiful Soup [[jupyter-notebook](https://github.com/senderle/hilt-text-analysis/blob/master/Beautiful%20Soup.ipynb)]
* **Session 2: Preparing Data**
    1. Preparing texts with NLTK [[jupyter-notebook](https://github.com/senderle/hilt-text-analysis/blob/master/Preparing%20Texts.ipynb)]
* **Session 3: Counting Words**
    1. Counting words with Voyant [[pdf](add_link) [webapp](http://voyant-tools.org/)]
    2. Word frequencies with NLTK [[jupyter-notebook](add_link)]
    3. Enhanced slave narratve metadata [[csv](https://github.com/senderle/hilt-text-analysis/blob/master/enhanced-metadata.csv)]
* **Session 4: Context and Order**
    1. Concordances with AntConc [[desktop-app](http://www.laurenceanthony.net/software/antconc/)]

## **Day 3:**
* **Session 1: Second Order Methods**
    1. Combining word count and context
* **Session 2: Topic Modeling**
    1. Topic modeling [[pdf](https://github.com/senderle/hilt-text-analysis/blob/master/Topic%20Modeling%20Howto%20HILT%202016.pdf) [desktop-app](https://github.com/senderle/topic-modeling-tool) [data](https://github.com/senderle/hilt-text-analysis/raw/master/slave-narrative-topic-model.zip)]
* **Session 3: Text Categorization**
    1. Text categorization
    2. Logistic regression with WEKA [[pdf](https://github.com/senderle/hilt-text-analysis/blob/master/Logistic%20Regression%20with%20Weka%20-%20HILT%202016.pdf) [desktop-app](http://www.cs.waikato.ac.nz/ml/weka/) [csv](https://github.com/senderle/hilt-text-analysis/tree/master/paceofchange-flat)]

## **Day 4:**
* **Session 1: Natural Language Processing**
    1. Stanford NER [[desktop-app](http://nlp.stanford.edu/software/CRF-NER.shtml)]
* **Session 2: NLP Adventures**
    1. Conditional frequences with NLTK [[jupyter-notebook](https://github.com/senderle/hilt-text-analysis/blob/master/NLP%20Adventures.ipynb)]
    2. Women postwar NY
* **Session 3: Presenting Results**
    1. Preparing results
    2. Visualization
    3. Thinking about audience
