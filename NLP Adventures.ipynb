{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the Natural Language Toolkit (NLTK) to prepare a text. (And this afternoon to do some analysis.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we will import the text of a corpus. This time we are using a subset because NLP is resource intensive. We could run the entire Slave Narrative corpus; however, it takes more time to process than we have. The subset is of women-authored texts published in the state of New York after the Civil War."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tell it where we want the text to come from. Use the path to get to the women_ny_postwar folder of texts. For this notebook to work, you'll need to click into the cell below and change the root location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_root = '/Users/kalle/Desktop/HILT 2016/data/women_ny_postwar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to read in the files from that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = PlaintextCorpusReader(corpus_root,  '.*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will keep capital letters, but remove punctuation and tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_corpus = corpus.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_corpus = [w for w in words_corpus if w.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to tag up with part of speech in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_corpus = nltk.pos_tag(words_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a sample of what we have produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('and', 'CC'),\n",
       " ('will', 'MD'),\n",
       " ('remain', 'VB'),\n",
       " ('forever', 'RB'),\n",
       " ('impossible', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('adequately', 'RB'),\n",
       " ('portray', 'VB'),\n",
       " ('its', 'PRP$'),\n",
       " ('unspeakable', 'JJ'),\n",
       " ('horrors', 'NNS'),\n",
       " ('its', 'PRP$'),\n",
       " ('heartbreaking', 'NN'),\n",
       " ('sorrows', 'VBZ'),\n",
       " ('its', 'PRP$'),\n",
       " ('fathomless', 'JJ'),\n",
       " ('miseries', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('hopeless', 'JJ'),\n",
       " ('grief', 'NN'),\n",
       " ('its', 'PRP$'),\n",
       " ('intolerable', 'JJ'),\n",
       " ('shames', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('its', 'PRP$'),\n",
       " ('heaven', 'NN'),\n",
       " ('defying', 'NN'),\n",
       " ('and', 'CC')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_corpus[539:569]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this mean?  The results are in tuples -- pairs of words and tags. The tags have a key, which you can find here: http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/. NN is a singular or mass noun -- \"grief.\" NNS is a plural noun -- \"miseries.\" JJ is an adjective -- \"fathomless.\" VB is a verb -- \"portray.\" RB is an adverb -- \"adequately.\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we could create a term frequency distribution table, we can also create a tag frequency table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_fd = nltk.FreqDist(tag for (word, tag) in pos_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort by how common a part of speech is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 31566),\n",
       " ('IN', 29540),\n",
       " ('DT', 22574),\n",
       " ('PRP', 20743),\n",
       " ('VBD', 16170),\n",
       " ('NNP', 16144),\n",
       " ('JJ', 14141),\n",
       " ('RB', 11010),\n",
       " ('CC', 10835),\n",
       " ('VB', 10569)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_fd.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the prevalence of \"little\" in women's narratives that we saw in our discussion on Tuesday, I would like to know more about the kinds of adjectives that these writers were using.  Adjectives are denoted by JJ, JJR, and JJS.\n",
    "\n",
    "First, we make a frequency distribution for the words in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_fd = nltk.FreqDist(word for (word, tag) in pos_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 11677),\n",
       " ('and', 8606),\n",
       " ('to', 7742),\n",
       " ('of', 7015),\n",
       " ('I', 5388),\n",
       " ('a', 4277),\n",
       " ('in', 4079),\n",
       " ('was', 3750),\n",
       " ('that', 2874),\n",
       " ('her', 2685)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_fd.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Oh hai, stopwords.)\n",
    "\n",
    "Then we are going to employ the conditional frequency distribution again. This time the condition will be being an adjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd_pos = nltk.ConditionalFreqDist((tag, word) for (word, tag) in pos_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('old', 445),\n",
       " ('good', 371),\n",
       " ('many', 349),\n",
       " ('great', 314),\n",
       " ('little', 305),\n",
       " ('poor', 278),\n",
       " ('other', 277),\n",
       " ('own', 236),\n",
       " ('much', 216),\n",
       " ('last', 215)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd_pos['JJ'].most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we're going to store these word frequency counts in a file for use elsewhere. We'll store them in a CSV file that many different kinds of software can open. Python provides support for CSV files in the `csv` library, so first we import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save word frequency counts per document for the 200 most common adjectives to our CSV. First, we create a conditional frequency distribution in which the condition is the filename in which the word appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_fileids = [fileid for fileid in corpus.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileid_frequency = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for fileid in corpus.fileids():\n",
    "    for word in corpus.words(fileid):\n",
    "        fileid_frequency.append((fileid, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_data = nltk.ConditionalFreqDist(fileid_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_adjectives = [word for word, tag in cfd_pos['JJ'].most_common(200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('common-adjectives.csv', 'w') as output:\n",
    "    csv_writer = csv.writer(output)\n",
    "    header = ['Filename']\n",
    "    header.extend(common_adjectives)\n",
    "    csv_writer.writerow(header)\n",
    "    for fileid in corpus.fileids():\n",
    "        row = [fileid]\n",
    "        row.extend(csv_data[fileid][w] for w in common_adjectives)\n",
    "        csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd_pos_new = nltk.ConditionalFreqDist(pos_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('corpus-pos.csv', 'w') as output:\n",
    "    csv_writer = csv.writer(output)\n",
    "    header = ['ID', 'Word', 'POS', 'Count']\n",
    "    csv_writer.writerow(header)\n",
    "    row_id = 0\n",
    "    for word in cfd_pos_new:\n",
    "        for pos, count in cfd_pos_new[word].items():\n",
    "            row = [row_id, word, pos, count]\n",
    "            csv_writer.writerow(row)\n",
    "            row_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
